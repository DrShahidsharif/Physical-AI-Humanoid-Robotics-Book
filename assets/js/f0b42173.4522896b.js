"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[7514],{1754:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var s=i(4164);const t={diagramContainer:"diagramContainer_DHRx",diagramTitle:"diagramTitle_IGKs",diagramContent:"diagramContent_jjxD"};var a=i(4848);function o({title:e,children:n}){return(0,a.jsxs)("div",{className:(0,s.A)("diagram-container",t.diagramContainer),children:[e&&(0,a.jsx)("h4",{className:t.diagramTitle,children:e}),(0,a.jsx)("div",{className:t.diagramContent,children:n})]})}},2162:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var s=i(4164);const t={learningGoalsContainer:"learningGoalsContainer_TsnZ",learningGoalsHeader:"learningGoalsHeader_gueA",learningGoalsContent:"learningGoalsContent_zywi"};var a=i(4848);function o({children:e}){return(0,a.jsxs)("div",{className:(0,s.A)("learning-goals",t.learningGoalsContainer),children:[(0,a.jsx)("div",{className:t.learningGoalsHeader,children:(0,a.jsx)("h3",{children:"\ud83c\udfaf Learning Goals"})}),(0,a.jsx)("div",{className:t.learningGoalsContent,children:e})]})}},5787:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>d,default:()=>g,frontMatter:()=>c,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"vla-robotics/index","title":"Vision-Language-Action Robotics (VLA)","description":"Overview","source":"@site/docs/vla-robotics/index.md","sourceDirName":"vla-robotics","slug":"/vla-robotics/","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla-robotics/","draft":false,"unlisted":false,"editUrl":"https://github.com/DrShahidsharif/Physical-AI-Humanoid-Robotics-Book/tree/main/docs/vla-robotics/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Vision-Language-Action Robotics (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Advanced ROS 2 Topics","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/ros2-nervous-system/advanced-topics"},"next":{"title":"VLA Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla-robotics/vla-fundamentals"}}');var t=i(4848),a=i(8453),o=i(2162),r=i(7412),l=i(1754);const c={sidebar_position:1,title:"Vision-Language-Action Robotics (VLA)"},d="Vision-Language-Action Robotics (VLA)",h={},u=[{value:"Overview",id:"overview",level:2},{value:"Learning Goals",id:"learning-goals",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Lab Exercise: Basic VLA System",id:"lab-exercise-basic-vla-system",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"vision-language-action-robotics-vla",children:"Vision-Language-Action Robotics (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) robotics represents the cutting edge of AI-powered robotic systems, where robots can understand natural language commands, perceive their environment visually, and execute complex actions to achieve goals. This paradigm enables more intuitive human-robot interaction and more flexible robotic capabilities. This module explores the integration of vision, language, and action systems in modern robotics."}),"\n",(0,t.jsx)(n.h2,{id:"learning-goals",children:"Learning Goals"}),"\n","\n",(0,t.jsx)(o.A,{children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the architecture of VLA robotic systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement vision-language models for robotic applications"}),"\n",(0,t.jsx)(n.li,{children:"Create action generation systems for robot control"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multimodal AI systems for complex tasks"}),"\n"]})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"VLA robotics combines three key AI capabilities:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Understanding visual input from cameras and sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Processing natural language commands and queries"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Generating appropriate robot behaviors and movements"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This integration enables robots to perform tasks based on high-level instructions rather than pre-programmed sequences."}),"\n",(0,t.jsx)(n.h2,{id:"lab-exercise-basic-vla-system",children:"Lab Exercise: Basic VLA System"}),"\n","\n",(0,t.jsxs)(r.A,{title:"Vision-Language-Action Pipeline",children:[(0,t.jsx)(n.p,{children:"In this lab, you will create a basic VLA system that can interpret simple language commands and execute corresponding actions."}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Python environment with AI libraries, robotic simulator"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up vision-language model"}),"\n",(0,t.jsx)(n.li,{children:"Create action mapping system"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with robot simulator"}),"\n",(0,t.jsx)(n.li,{children:"Test with natural language commands"}),"\n"]})]}),"\n","\n",(0,t.jsx)(l.A,{title:"VLA Robotics Architecture",children:(0,t.jsx)(n.p,{children:"// This would contain an architectural diagram showing VLA components\n// Placeholder for VLA architecture diagram"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"This module will guide you through creating sophisticated VLA systems that can understand complex instructions and execute multi-step tasks. We'll explore state-of-the-art models, integration techniques, and practical applications in real robotic systems."})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},7412:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var s=i(4164);const t={labExerciseContainer:"labExerciseContainer_wBKe",labTitle:"labTitle_t11A",labIcon:"labIcon_xYT5",labTitleText:"labTitleText_Cn5p",labContent:"labContent_K1D1"};var a=i(4848);function o({title:e,children:n}){return(0,a.jsxs)("div",{className:(0,s.A)("lab-exercise",t.labExerciseContainer),children:[(0,a.jsxs)("div",{className:(0,s.A)("lab-title",t.labTitle),children:[(0,a.jsx)("span",{className:t.labIcon,children:"\ud83e\uddea"}),(0,a.jsx)("h3",{className:t.labTitleText,children:e})]}),(0,a.jsx)("div",{className:t.labContent,children:n})]})}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var s=i(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);